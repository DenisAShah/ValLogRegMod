---
title: "Development and validation of logistic regression risk prediction models"
always_allow_html: true
output:
  github_document:
    toc: true
    toc_depth: 5
  keep_text: true
  pandoc_args: --webtex
---

## Steps
  
The steps taken in this file are:   
1. To develop a logistic regression risk prediction model.  
2. To assess the performance of the model in terms of calibration, discrimination and overall prediction error. We calculate the apparent, internal (optimism-corrected) validation and the external validation.  
3. To assess the potential clinical utility the model using decision curve analysis.  

### Installing and loading packages and import data

The following libraries are used in this file, the code chunk below will a) check whether you already have them installed, b) install them for you if not already present, and c) load the packages into the session.

```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  fig.retina = 3,
  fig.path = "imgs/03_PredLogReg_py/",
  echo = FALSE
)
```

<details>
  <summary>Click to expand code</summary>
```{r, wdlib_r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
# Use pacman to check whether packages are installed, if not load
if (!require("pacman")) install.packages("pacman")
library(pacman)

pacman::p_load(
  reticulate,
  tidyverse,
  knitr,
  kableExtra
)
```
</details>
```{r, wdlib_r, warning=FALSE, fig.align='center', eval=TRUE}
```

<details>
  <summary>Click to expand code</summary>
```{python, wdlib, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
# Load libraries and data
import warnings
warnings.simplefilter(action = "ignore", category = FutureWarning)
warnings.filterwarnings("ignore", category = RuntimeWarning) # suppressing warnings
import pandas as pd
import numpy as np
import scipy as sp
import tableone as tb
import statsmodels.api as smf
import matplotlib.pyplot as plt
import sklearn as sk
import seaborn as sns


# Get work directory
# os.getcwd()
url_rdata = "https://raw.githubusercontent.com/danielegiardiello/ValLogRegMod/main/Data/rdata.csv"
url_vdata = "https://raw.githubusercontent.com/danielegiardiello/ValLogRegMod/main/Data/vdata.csv"
# NOTE: go to 
# "https://github.com/danielegiardiello/ValLogRegMod/blob/main/Data/vdata.csv"
# then click" Raw" button to the upper right corner of the file preview.
# Copy and paste the url link to have the raw gitHub version of the data
rdata = pd.read_csv(url_rdata)
vdata = pd.read_csv(url_vdata)
# Inspect data:
# print(rdata.head(5)) # print the first five rows
# print(vdata.head(5)) # print the first five rows
# rdata.info() # inspect data as in R str()
# vdata.info() # inspect data as in R str()

## Data manipulation ----
# Development data 
# Converting categorical variables to dummies
rdata = pd.get_dummies(data = rdata, 
                       columns = ["ter_pos", "preafp", "prehcg"])
# Dropping columns not needed
rdata.drop(["ter_pos_No", "preafp_No", "prehcg_No"], 
           axis = 1, inplace = True)

# Validation data 
vdata = pd.get_dummies(data = vdata, 
                       columns=["ter_pos", "preafp", "prehcg"])
# Dropping columns not needed
vdata.drop(["ter_pos_No", "preafp_No", "prehcg_No"],
            axis = 1,
            inplace = True)
```
</details>

```{python, wdlib, message=FALSE, warning=FALSE, eval=TRUE}
```

### Data description
Men with metastatic non-seminomatous testicular cancer can often be cured nowadays by cisplatin based chemotherapy. After chemotherapy, surgical resection is a generally accepted treatment to remove remnants of the initial metastases, since residual tumor may still be present. In the absence of tumor, resection has no therapeutic benefits, while it is associated with hospital admission, and risks of permanent morbidity and mortality. Logistic regression models were developed to predict the presence of residual tumor, combining  well-known predictors, such as the histology of the primary tumor, pre-chemotherapy levels of tumor markers, and (reduction in) residual mass size.  
We first consider a data set (rdata) with 544 patients to develop a prediction model that includes 5 predictors. We then extend this model with the pre-chemotherapy level of the tumor marker lactate dehydrogenase (LDH). This illustrates ways to assess the incremental
value of a marker. LDH values were log transformed, after standardizing by dividing by the local upper levels of normal values, after examination of non-linearity with restricted cubic spline functions.  

We  first consider a data set with 544 patients to develop a prediction model that includes 5 predictors (rdata).  In a later study, we externally validated the 5 predictor model in 273 patients from a tertiary referral center (vdata). We illustrate ways to assess the usefulness of a model in a new setting.  
We then extend the developed model with the pre-chemotherapy level of the tumor marker lactate dehydrogenase (LDH). Since the validation data (vdata) did not have information about LDH, we assess the prediction performances of the basic model in the development (rdata) and in the validation data (vdata). 
Thus, we loaded the development data (rdata) and the validation data (vdata).    
More details about development and validation data are provided in the manuscript ["Assessing the performance of prediction models: a framework for some traditional and novel measures"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/) by Steyerberg et al. (2010).

#### Descriptive statistics

```{python, import, message=F, warning=F, comment=F}
# Get work directory
# os.getcwd()
url_rdata = "https://raw.githubusercontent.com/danielegiardiello/ValLogRegMod/main/Data/rdata.csv"
url_vdata = "https://raw.githubusercontent.com/danielegiardiello/ValLogRegMod/main/Data/vdata.csv"
# NOTE: go to 
# "https://github.com/danielegiardiello/ValLogRegMod/blob/main/Data/vdata.csv"
# then click" Raw" button to the upper right corner of the file preview.
# Copy and paste the url link to have the raw gitHub version of the data
rsel = pd.read_csv(url_rdata)
rsel = rsel.assign(dt = "Development data")

vsel = pd.read_csv(url_vdata)
vsel = vsel.assign(dt = "Validation data")

# Combine data
cdata = pd.concat([rsel, vsel], axis = 0)
cdata = cdata.reset_index()
columns = ["tum_res", "ter_pos", "preafp", "prehcg", "sqpost", "reduc10", "lnldhst"]
groupby = ["dt"]
categorical = ["tum_res","ter_pos","preafp", "prehcg"]
nonnormal = ["sqpost","reduc10","lnldhst"]
min_max = ["sqpost","reduc10","lnldhst"]

labels = {'tum_res': 'Residual tumor resection',
 'ter_pos': 'Primary tumor teratoma positive', 
 'preafp' : 'Elevated prechemotherapy AFP',
 'prehcg' : 'Elevated Prechemotherapy HCG',
 'sqpost' : 'Square root of mass size',
 'reduc10': 'Reduction in mass size per 10%' ,
 'lnldhst': 'log(LDH)'
}
        
Table1 = tb.TableOne(cdata, 
                  columns = columns, 
                  categorical = categorical, 
                  groupby = groupby, 
                  nonnormal = nonnormal, 
                  min_max = min_max,
                  rename = labels, 
                  missing = False,
                  pval = False,
                  remarks = False,
                  overall = False)

print(Table1.tabulate(tablefmt = "fancy_grid"))   
```


## Goal 1 - Develop a logistic regression risk prediction model

### 1.1 Check non-linearity of continuous predictors

Here we investigate the potential non-linear relation between continuous predictors and the outcomes. We apply three-knot restricted cubic splines  (details are given in e.g. Frank Harrell's book 'Regression Model Strategies (second edition)', page 27. We assess the potential non-linearity graphically (plotting the two continuous predictors against the log odds (XB or linear predictor) of both event types. Also, we compare the models with and without splines based on the AIC.

<details>
  <summary>Click to expand code</summary>
```{python, ff, warning=FALSE, fig.align='center', eval=FALSE, echo=TRUE}

# Formula restricted cubic spline
# k = #knots
# X1 = X
# X_j+1 = (X - t_j)_+**3 - (X - t_k-1)_+**3 * (t_k - t_j) / (t_k - t_k-1)
# + (X - t_k)_+**3 (t_k-1 - t_j)((t_k - t_k-1))
# Models without splines

# Models with splines
# Create function to calculate restricted cubic splines with three knots
def rcs_3(x):
    res_x = np.zeros((len(rdata), 1))
    qknots = [.1, .5, .9]
    knots = np.quantile(x, q = qknots)
    res_x[:, 0] = (np.power(np.clip((x - knots[0]), a_min = 0, a_max = None), 3) - np.power(np.clip((x - knots[1]), a_min = 0, a_max = None), 3) *((knots[2] - knots[0])/(knots[2] - knots[1])) + np.power(np.clip((x - knots[2]), a_min = 0, a_max = None), 3) * ((knots[1] - knots[0])/(knots[2] - knots[1]))) / ((knots[2] - knots[0])**2)
    return(res_x)
# NOTE: to be extended for 4,5, 6 and 7 knots

# Add splines to data frame
rdata["sq_rcs1"] = rcs_3(rdata.sqpost)
rdata["reduc10_rcs1"] = rcs_3(rdata.reduc10)

# Predictors data with splines
X =  rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", 
            "sqpost", "sq_rcs1", "reduc10", "reduc10_rcs1"]]
X = X.assign(intercept = 1.0)

# Predictors without splines
X2 =  rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", 
            "sqpost", "reduc10"]]
X2 = X2.assign(intercept = 1.0)
 
# Fitting Generalised linear model on transformed dataset
fit_rcs = smf.GLM(rdata.tum_res, X,  family = smf.families.Binomial()).fit()
fit = smf.GLM(rdata.tum_res, X2, family = smf.families.Binomial()).fit()

# Save predictors of the validation model
coeff_rcs = fit_rcs.params
cov_rcs = X         

# Calculating the linear predictor (X*beta)
lp_rcs = np.matmul(cov_rcs, coeff_rcs)

# Non-lineary of sqpost adjusted for the other predictors
cov_rcs = cov_rcs.assign(ter_pos_Yes = 0,
                         preafp_Yes = 0,
                         prehcg_Yes = 0,
                         reduc10 = np.median(rdata.reduc10),
                         reduc10_rcs1 = np.median(rdata.reduc10_rcs1))


# Calculating the lp of sqpost adjusted for the other predictors
lp_rcs_sq = np.matmul(cov_rcs, coeff_rcs)

# Calculating standard errors
vcov = fit_rcs.cov_params()

# Matrix X *%* vcov *%* t(X) (sqrt of the diagonal)
std_err = np.power(np.diagonal(cov_rcs.dot(vcov).dot(pd.DataFrame.transpose((cov_rcs)))), 1/2)

# Save to df
alpha = 0.05
df_rcs_sq = pd.DataFrame(
  {"sq" : rdata.sqpost,
   "lp_rcs_sq" : lp_rcs_sq,
   "std_err" : std_err,
   'lower_95' :  lp_rcs_sq - sp.stats.norm.ppf(1 - alpha / 2) * std_err,
   'upper_95' : lp_rcs_sq + sp.stats.norm.ppf(1 - alpha / 2) * std_err}
)

# Sorting by sqpost
df_rcs_sq = df_rcs_sq.sort_values(by = ['sq']) # sort
# df_rcs = pd.Series({c: df_rcs[c].unique() for c in df_rcs}) # unique values

# Non-lineary assessment of reduc10 adjusted for the other covariates
cov_rcs = cov_rcs.assign(ter_pos_Yes = 0,
                         preafp_Yes = 0,
                         prehcg_Yes = 0,
                         sqpost = np.median(rdata.sqpost),
                         sq_rcs1 = np.median(rdata.sq_rcs1),
                         reduc10 = rdata.reduc10,
                         reduc10_rcs1 = rdata.reduc10_rcs1)

lp_rcs_reduc10 = np.matmul(cov_rcs, coeff_rcs)

# Matrix X *%* vcov *%* t(X) (sqrt of the diagonal)
std_err = np.power(np.diagonal(cov_rcs.dot(vcov).dot(pd.DataFrame.transpose((cov_rcs)))), 1/2)

# Save to df
alpha = 0.05
df_rcs_rd = pd.DataFrame(
  {"reduc10" : rdata.reduc10,
   "lp_rcs_reduc10" : lp_rcs_reduc10,
   "std_err" : std_err,
   'lower_95' :  lp_rcs_reduc10 - sp.stats.norm.ppf(1 - alpha / 2) * std_err,
   'upper_95' : lp_rcs_reduc10 + sp.stats.norm.ppf(1 - alpha / 2) * std_err}
)

df_rcs_rd = df_rcs_rd.sort_values(by = ['reduc10']) # sort

# Plotting
fig, (ax1, ax2) = plt.subplots(1, 2)

# First plot - predictor: sq ---
ax1.plot(df_rcs_sq.sq, df_rcs_sq.lp_rcs_sq, "-", 
         color = "black")
ax1.plot(df_rcs_sq.sq, df_rcs_sq.lower_95, "--", 
         color = "black")
ax1.plot(df_rcs_sq.sq, df_rcs_sq.upper_95, "--", 
         color = "black")
plt.setp(ax1, xlabel = 'Square root of postchemotherapy mass size')
plt.setp(ax1, ylabel = 'log Odds')

# Second plot - predictor: reduc10 ---
ax2.plot(df_rcs_rd.reduc10, df_rcs_rd.lp_rcs_reduc10, "-", 
         color = "black")
ax2.plot(df_rcs_rd.reduc10, df_rcs_rd.lower_95, "--", 
         color = "black")
ax2.plot(df_rcs_rd.reduc10, df_rcs_rd.upper_95, "--", 
         color = "black")
plt.setp(ax2, xlabel = 'Reduction in mass size per 10%')
plt.show()
plt.clf()
plt.cla()
plt.close('all')
```
</details>

```{python, ff, warning=FALSE, fig.align='center', eval=TRUE}
```

```{python, res_aic, fig.align='center'}
df_aic = pd.DataFrame({
  "AIC without splines" : fit.aic,
  "AIC with splines" : fit_rcs.aic
},
  index = [0]
)

# print(df_aic.to_markdown())
```

```{r, res_aic_r, fig.align='center'}
kable(py$df_aic, row.names = FALSE) |>
  kable_styling("striped", position = "center")
```
Both the graphical comparison and the AIC comparison suggested no relevant departure from linear relations between the continuous predictors (square root of post-chemotherapy mass size and reduction in mass size) and the risk of residual tumor at post-chemotherapy resection.  

### 1.2 Examine the fit of the models

+ Logistic regression risk prediction model without LDH

```{python, summary_lrm1, fig.align='center',warning=FALSE}
X =  rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", 
            "sqpost", "reduc10"]]
X.insert(0, 'intercept', 1.0)

fit_lrm = smf.GLM(rdata.tum_res, X, 
                   family = smf.families.Binomial()).fit()
fit_lrm.summary()
```

+ Logistic regression risk prediction model with LDH
```{python, summary_lrm2, fig.align='center',warning=FALSE}
X =  rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", 
            "sqpost", "reduc10", "lnldhst"]]
X.insert(0, 'intercept', 1.0)

fit_lrm_ldh = smf.GLM(rdata.tum_res, X, 
                   family = smf.families.Binomial()).fit()
fit_lrm_ldh.summary()
```

The coefficients of the models indicated that positive tumor teratoma, elevated prechemoterapy AFP levels, elevated prechemoterapy HCG levels, postchemotherapy mass size (mm) (expressed in square root) are associated with higher risk to residual tumor after resection. Reduction in mass size is associated with a reduced risk to have residual tumor after resection.

### 1.3 Plot of predictors vs estimated in the validation data

To get further insight into the effect of the covariates, we plot the covariate values observed in the validation set against the estimated absolute risk of having residual tumor after resection. This gives an idea of the size of the effects.

<details>
  <summary>Click to expand code</summary>
```{python, plot_risk, fig.align='center',warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}
# Models -------------
X = rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X = X.assign(intercept = 1.0)

fit_lrm = smf.GLM(rdata.tum_res, X, family = smf.families.Binomial()).fit()

# Predictors - validation data                
X_val =  vdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X_val = X_val.assign(intercept = 1.0)

# Predicted probabilities estimated by the model in the validation data
vdata = vdata.assign(pred = fit_lrm.predict(X_val))

fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 3)
fig.tight_layout(pad = 2.5) 
# Or:
# subplots_adjust(left = None, bottom = None, right = None, top = None, 
#                 wspace = None, hspace=None) # to modify margins
#

# Positive teratoma tumor
sns.boxplot(x = "ter_pos_Yes", y = "pred", 
            data = vdata, palette = "Blues", ax = ax1[0]).set(
      xlabel = "Positive teratoma tumor",
      ylabel = "Estimated risk"
            )
            
## Elevated AFP levels           
sns.boxplot(x = "preafp_Yes",
            y = "pred",
            data = vdata, palette = "Blues", ax = ax1[1]).set(
              xlabel = "Elevated AFP levels",
              ylabel = "Estimated risk"
            )
            
# Elevated HCG
sns.boxplot(x = "prehcg_Yes",
            y = "pred",
            data = vdata, palette = "Blues", ax = ax1[2]).set(
              xlabel = "Elevated HCG levels",
              ylabel = "Estimated risk"
)
            
# Square root of postchemotherapy mass size
# Perform lowess
lowess = smf.nonparametric.lowess
fit_lowess = lowess(vdata.sqpost,
                    vdata.pred,
                    frac = 2/3,
                    it = 0) # same f and iter parameters as R
sns.scatterplot(x = "sqpost",
                y = "pred",
                data = vdata, ax = ax2[0])
ax2[0].set_xlim(0, 20)
ax2[0].set_ylim(0, 1.2)
ax2[0].set_xlabel("Square root of postchemotherapy mass size", fontsize = 8)
ax2[0].set_ylabel("Estimated risk")
ax2[0].plot(fit_lowess[:, 1], fit_lowess[:, 0], "-", color = "red")

           
# Reduction mass size
# Perform lowess
lowess = smf.nonparametric.lowess
fit_lowess = lowess(vdata.reduc10,
                    vdata.pred, 
                    frac = 2/3,
                    it = 0) # same f and iter parameters as R
sns.scatterplot(x = "reduc10",
                y = "pred",
                data = vdata, ax = ax2[1]).set(
                  xlabel = "Reduction mass size",
                  ylabel = "Estimated risk"
)
ax2[1].set_xlim(0, 10)
ax2[1].set_ylim(0, 1.2)
ax2[1].plot(fit_lowess[:, 1], fit_lowess[:, 0], "-", color = "red")
plt.show()
plt.clf()
plt.cla()
plt.close('all')
```
</details>

```{python, plot_risk, fig.align='center', warning=FALSE, message=FALSE, eval=TRUE}
```

## Goal 2 - Assessing performance of a logistic regression risk prediction model

Here we evaluate the performance of the prediction model in terms of discrimination, calibration and overall prediction error. 
We assess the prediction performance of the developed model not including LDH internally and in an external data.   


### 2.1 Discrimination

We here calculate:  

+ The c-statistic: it is a rank order statistic for predictions against true outcomes. The concordance (c) statistic is the most commonly used performance measure to indicate the discriminative ability of generalized linear regression models. For a binary outcome, c is identical to the area under the Receiver Operating Characteristic (ROC) curve, which plots the sensitivity (true positive rate) against 1 – (false positive rate) for consecutive cutoffs for the probability of an outcome. Accurate predictions discriminate between those with and those without the outcome.

+ Discrimination slope: it can be used as a simple measure for how well subjects with and without the outcome are separated. It is calculated as the absolute difference in average predictions for those with and without the outcome. Visualization is readily possible with a box plot or a histogram, which will show less overlap between those with and those without the outcome for a better discriminating model. 
 

More details are in ["Assessing the performance of prediction models: a framework for some traditional and novel measures"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/) by Steyerberg et al. (2010);


<details>
  <summary>Click to expand code</summary>
```{python, discrimination, warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}

## Fitting the logistic regression model ------------------
# Logistic regression using statsmodels library
y = rdata["tum_res"]
X_rdata = rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X_rdata = X_rdata.assign(intercept = 1.0)

lrm = smf.GLM(y, X_rdata, family = smf.families.Binomial())
result_lrm = lrm.fit()

# Create dataframe dev_out and val_out containing all info useful
# to assess prediction performance in the development and in the validation data

# Save estimated predicted probabilities in the development data
pred_rdata = result_lrm.predict(X_rdata)

# Save predictors of the validation model
X_vdata = vdata         
X_vdata = X_vdata.assign(intercept = 1.0)
X_vdata = X_vdata[["ter_pos_Yes", "preafp_Yes","prehcg_Yes", "sqpost", "reduc10", "intercept"]]

# Save estimated predicted probabilities in the validation data
pred_vdata = result_lrm.predict(X_vdata)

# Save coefficients of the developed model
coeff = result_lrm.params

# Calculating the linear predictor (X*beta)
lp_rdata = np.matmul(X_rdata, coeff)
lp_vdata = np.matmul(X_vdata, coeff)

# Create the dataframe including all useful info 
# y_dev = outcome of the development
# lp_dev = linear predictor calculated in the developement data
# pred_dev = estimated predicted probability in the development data
dev_out =  pd.DataFrame({'y_dev': rdata["tum_res"], 
                         'lp_dev' : lp_rdata,
                         'pred_dev' : pred_rdata})                      
dev_out = dev_out.assign(intercept = 1.0) # Add intercept

# Validation data --
# y_val = outcome of the validation data
# lp_val = linear predictor calculated in the validation data
# pred_val = estimated predicted probability in the validation data
val_out =  pd.DataFrame({'y_val': vdata["tum_res"], 
                         'lp_val' : lp_vdata,
                         'pred_val' : pred_vdata})                      
val_out = val_out.assign(intercept = 1.0) # Add intercept


# Discrimination -------------------

import lifelines 
from lifelines.utils import concordance_index


## C-statistic ---
cstat_rdata = concordance_index(dev_out.y_dev, dev_out.lp_dev) # development data
cstat_vdata = concordance_index(val_out.y_val, val_out.lp_val) # validation data

## Discrimination slope ---

### Development data ----
#### Apparent validation
dev_out_group = dev_out.groupby("y_dev").mean()
dslope_rdata = abs(dev_out_group.pred_dev[1] - dev_out_group.pred_dev[0])

### Validation data ----
val_out_group = val_out.groupby("y_val").mean()
dslope_vdata = abs(val_out_group.pred_val[1] - val_out_group.pred_val[0])

#### Bootstrap percentile confidence intervals
# Bootstrap confidence intervals for development and validation set
# NOTE: I need to understand how to set up a random seed to reproduce
# the same boostrapped data
B = 2000
bdev_out = {}
bval_out = {}
cstat_dev_b = [0] * B
dev_bgroup = {}
dslope_dev_b = [0] * B
cstat_val_b = [0] * B
val_bgroup = {}
dslope_val_b = [0] * B
for j in range(B): 
  
  bdev_out[j] = sk.utils.resample(dev_out, 
      replace = True, 
      n_samples = len(dev_out)) # bootstrapping development data
      
  bval_out[j] = sk.utils.resample(val_out, 
      replace = True, 
      n_samples = len(val_out)) # bootstrapping validation data
  
  # Bootstrapped discrimination measures - development data     
  cstat_dev_b[j] = concordance_index(bdev_out[j].y_dev, bdev_out[j].lp_dev)
  dev_bgroup[j] = bdev_out[j].groupby("y_dev").mean().pred_dev
  dslope_dev_b[j] = abs(dev_bgroup[j][1] - dev_bgroup[j][0])
  
  # Bootstrapped discrimination measures - validation data  
  cstat_val_b[j] = concordance_index(bval_out[j].y_val, bval_out[j].lp_val)
  val_bgroup[j] = bval_out[j].groupby("y_val").mean().pred_val
  dslope_val_b[j] = abs(val_bgroup[j][1] - val_bgroup[j][0])
  
# Internal validation function
#
# Calculate optimism-corrected bootstrap internal validation for c-statistic and 
# discrimination slope.
# 
# @type  data: pandas.core.frame.DataFrame
# @param data: data frame
# @type  y:  str 
# @param y:  variabile identifying the binary outcome
# @type  X:  pandas.core.frame.DataFrame
# @param X:  predictors including the intercept
# @type  B:  integer
# @param B:  number of bootstrap sample (default B = 2000)
# @rtype:    pandas.core.frame.DataFrame 
# @return:   pandas.core.frame.DataFrame including optimism-corrected bootstrap internal
#            validation prediction performance measures.
# 

def bootstrap_cv_lrm(data, y, X, B = 2000):

  bdata = {}
  X_boot = {}
  lrm_boot = {}
  coeff_boot = {}
  lp_boot = {}
  lp_orig = {}
  pred_boot = {}
  pred_orig = {}
  cstat_boot = [0] * B
  cstat_orig = [0] * B
  boot_out_group = {}
  orig_out_group = {}
  dslope_boot = [0] * B
  dslope_orig = [0] * B
  
  # Predictors of original (development) data
  X_rdata = data[X]
  X_rdata = X_rdata.assign(intercept = 1.0)
  # Run original model
  lrm_app = smf.GLM(data[y], X_rdata, family = smf.families.Binomial()).fit()
  # Save predictors of the original model
  coeff_apparent = lrm_app.params
  # Calculating the linear predictor (X*beta)
  lp_apparent = np.matmul(X_rdata, coeff_apparent)
  pred_apparent = lrm_app.predict(X_rdata)
  # Apparent C-statistic and discrimination slope
  cstat_app = concordance_index(data[y], lp_apparent)  # c-statistic
  app_out_group = pred_apparent.groupby(data[y]).mean()
  dslope_app = abs(app_out_group[1] - app_out_group[0])
  
  for j in range(B):
    # Bootstrapping development data
    bdata[j] = sk.utils.resample(data, replace = True, n_samples = len(data)) 
    bdata[j] = bdata[j].assign(intercept = 1.0)
    X_boot[j] =  bdata[j][pd.Index(X_rdata.columns)]
    # Logistic regression model in every bootstrapped data
    lrm_boot[j] = smf.GLM(bdata[j][y], X_boot[j], family = smf.families.Binomial())
    coeff_boot[j] = lrm_boot[j].fit().params # coefficients
    # Linear predictor and predicted probabilities 
    # in every bootstrapped model and in every bootstrapped data
    lp_boot[j] = np.matmul(X_boot[j], coeff_boot[j])
    pred_boot[j] = lrm_boot[j].fit().predict(X_boot[j])
    # Linear predictor and predicted probabilities 
    # in every bootstrapped model in every data
    lp_orig[j] = np.matmul(X_rdata, coeff_boot[j])
    pred_orig[j] = lrm_boot[j].fit().predict(X_rdata)
    
    # Discrimination --
    ## C-statistic --
    cstat_boot[j] = concordance_index(bdata[j][y], lp_boot[j]) 
    cstat_orig[j] = concordance_index(rdata[y], lp_orig[j]) 
    cstat_diff = np.subtract(cstat_boot, cstat_orig)
    cstat_opt = np.mean(cstat_diff)
 
    ## Discrimination slope --
    boot_out_group[j] = pred_boot[j].groupby(bdata[j][y]).mean()
    dslope_boot[j] = abs(boot_out_group[j][1] - boot_out_group[j][0])
  
    orig_out_group[j] = pred_orig[j].groupby(rdata[y]).mean()
    dslope_orig[j] = abs(orig_out_group[j][1] - orig_out_group[j][0])
    dslope_diff = np.subtract(dslope_boot, dslope_orig)
    dslope_opt = np.mean(dslope_diff)
    
    res_int = pd.DataFrame(
      {"C-statistic": cstat_app - cstat_opt,
       "Discrimination slope": dslope_app - dslope_opt},
       index = [0]
    )
    
  return(res_int)

res_int = bootstrap_cv_lrm(rdata, 
                           y = "tum_res",
                           X = {"ter_pos_Yes", "preafp_Yes",
                                 "prehcg_Yes", "sqpost", "reduc10"},
                           B = 2000)
```
</details>

```{python, discrimination, fig.align='center', warning=FALSE, message=FALSE, eval=TRUE}
```

```{python, res_discr_py, warning=FALSE, message=FALSE}
# Save results
k = 2 # digits
res_discr = np.reshape(
  (round(cstat_rdata, 2),
   round(np.percentile(cstat_dev_b, q = 2.5), k),
   round(np.percentile(cstat_dev_b, q = 97.5), k), 
   
   round(np.float64(res_int["C-statistic"]), k),
   None,
   None,
   
   round(cstat_vdata, k),
   round(np.percentile(cstat_val_b, q = 2.5), k),
   round(np.percentile(cstat_val_b, q = 97.5), k),
   
   round(dslope_rdata, k),
   round(np.percentile(dslope_dev_b, q = 2.5), k),
   round(np.percentile(dslope_dev_b, q = 97.5), k),
   
   round(np.float64(res_int["Discrimination slope"]), k),
   None,
   None,
   
   round(dslope_vdata, k),
   round(np.percentile(dslope_val_b, q = 2.5), k),
   round(np.percentile(dslope_val_b, q = 97.5), k)),
   
   (2, 9)
)

res_discr = pd.DataFrame(res_discr, 
                         columns = np.tile(["Estimate", "2.5 %", "97.5 %"], 3) ,
                         index = ["C-statistic", "Discrimination slope"])
```


```{r, res_discr_r, fig.align='center'}
kable(py$res_discr, row.names = TRUE) |>
  kable_styling("striped", position = "center") |>
  add_header_above(c(" " = 1, 
                     "Apparent" = 3, 
                     "Internal" = 3, 
                     "External" = 3))

```

C-statistic was 0.82 (95% confidence interval, CI: 0.78-0.85), and 0.79 (95% CI: 0.72-0.84) in the development and validation data, respectively. Internal cross-validation based on optimism-corrected bootstrapping showed a C-statistic of 0.81.  

Discrimination slope was 0.30 (95% CI: 0.26-0.34), and 0.24 (95% CI: 0.18-0.30) for the development and validation data, respectively. Internal cross-validation based on optimism-corrected bootstrapping showed a discrimination slope of 0.29.

### 2.2 Calibration
Calibration refers to the agreement between observed outcomes and predictions. For example, if we predict a 20% risk of residual tumor for a testicular cancer patient, the observed frequency of tumor should be approximately 20 out of 100 patients with such a prediction.

Different level of calibration can be estimated: mean, weak, and moderate calibration according to the calibration hierarchy defined by Van Calster et al. [here](https://www.sciencedirect.com/science/article/pii/S0895435615005818).  

#### 2.2.1 Mean calibration
Mean calibration refers how systematically the model might under- or over- predicts the actual risk.   

The mean calibration can be estimated:  

+ using the Observed and Expected ratio. The observed number of events is the sum of the events (or cases) present in the data. The expected is estimated summing the predicted probability of the event estimated by the model. Ratio equals to 1 indicates perfect (mean) calibration, values lower or greater than 1 indicate over- and under- prediction, respectively.

+ calibration intercept (or calibration-in-the-large): indicates the extent that predictions are systematically too low or too high.  


<details>
  <summary>Click to expand code</summary>
```{python, mean_cal, fig.align='center',echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
## Fitting the logistic regression model ------------------
y = rdata["tum_res"]
X_rdata = rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X_rdata = X_rdata.assign(intercept = 1.0)

lrm = smf.GLM(y, X_rdata, family = smf.families.Binomial())
result_lrm = lrm.fit()


# Save predictors of the validation model
X_vdata = vdata         
X_vdata = X_vdata.assign(intercept = 1.0)
X_vdata = X_vdata[["ter_pos_Yes", "preafp_Yes","prehcg_Yes", "sqpost", "reduc10", "intercept"]]

# Save estimated predicted probabilities in the validation data
pred_vdata = result_lrm.predict(X_vdata)

# Save coefficients of the developed model
coeff = result_lrm.params

# Calculating the linear predictor (X*beta)
lp_vdata = np.matmul(X_vdata, coeff)

# Create the dataframe including all useful info
# Validation data --
# y_val = outcome of the validation data
# lp_val = linear predictor calculated in the validation data
val_out =  pd.DataFrame({'y_val': vdata["tum_res"], 
                         'lp_val' : lp_vdata})                      
val_out = val_out.assign(intercept = 1.0) # Add intercept

# Calibration intercept (calibration-in-the-large) -------------
# df_cal_int = pd.concat(y_val, lp_val)
cal_int = smf.GLM(val_out.y_val, 
                  val_out.intercept, 
                  family = smf.families.Binomial(),
                  offset = val_out.lp_val)
res_cal_int = cal_int.fit()
# np.float64(res_cal_int.params)
# res_cal_int.conf_int()
# np.float64(res_cal_int.conf_int()[0])


# Observed/Expected ratio
Obs = np.sum(val_out.y_val)
Exp = np.sum(val_out.pred_val)
OE = Obs / Exp
alpha = 0.05
res_OE = pd.DataFrame(
  {"OE" : OE,
   "lower": OE * np.exp(- sp.stats.norm.ppf(1 - alpha / 2) * np.power(1 / Obs, 0.5)),
   "upper": OE * np.exp(sp.stats.norm.ppf(1 - alpha / 2) * np.power(1 / Obs, 0.5))
   },
   index=[0]
)
```
</details>

```{python, mean_cal, fig.align='center', warning=FALSE, eval=TRUE}
```

```{python, res_mean_cal, fig.align='center',echo=FALSE}
k = 2 # digits
res_mean_cal = np.reshape(
  (
    round(np.float64(res_cal_int.params), k),
    round(np.float64(res_cal_int.conf_int()[0]), k),
    round(np.float64(res_cal_int.conf_int()[1]), k),
    
    round(np.float64(res_OE["OE"]), k),
    round(np.float64(res_OE["lower"]), k),
    round(np.float64(res_OE["upper"]), k)
  ),
  (2,3)
)

res_mean_cal = pd.DataFrame(res_mean_cal, 
                         columns = ["Estimate", "2.5 %", "97.5 %"] ,
                         index = ["Calibration intercept", "O/E ratio"])
```

```{r, res_mean_cal_r, fig.align='center',echo=FALSE}
kable(py$res_mean_cal) |>
  kable_styling("striped", position = "center") 
```

Both calibration intercept and O/E ratio showed good mean calibration.
The prediction model did not systematically over or underestimate the actual risk. 

#### 2.2.2 Weak calibration
The term ‘weak’ refers to the limited flexibility in assessing calibration. We are essentially summarizing calibration of the observed proportions of outcomes versus predicted probabilities using only two parameters i.e. a straight line. In other words, perfect weak
calibration is defined as mean calibration ratio and calibration slope of unity(or calibration intercept equals to zero). 
The calibration slope indicates the overall strength of the linear predictor (LP), which can be interpreted as the level of overfitting
(slope <1) or underfitting (slope>1). A value of slope smaller than 1 can also be interpreted as reflecting a need for shrinkage of regression coefficients in a prediction model. 
<details>
  <summary>Click to expand code</summary>
```{python, weak_cal, fig.align='center',echo=TRUE, eval=FALSE ,message=FALSE, warning=FALSE}

## Fitting the logistic regression model ------------------
y = rdata["tum_res"]
X_rdata = rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X_rdata = X_rdata.assign(intercept = 1.0)

lrm = smf.GLM(y, X_rdata, family = smf.families.Binomial())
result_lrm = lrm.fit()

# Save predictors of the validation model
X_vdata = vdata         
X_vdata = X_vdata.assign(intercept = 1.0)
X_vdata = X_vdata[["ter_pos_Yes", "preafp_Yes","prehcg_Yes", "sqpost", "reduc10", "intercept"]]

# Save coefficients of the developed model
coeff = result_lrm.params

# Calculating the linear predictor (X*beta)
lp_vdata = np.matmul(X_vdata, coeff)

# Create the dataframe including all useful info
# Validation data --
# y_val = outcome of the validation data
# lp_val = linear predictor calculated in the validation data
val_out =  pd.DataFrame({'y_val': vdata["tum_res"], 
                         'lp_val' : lp_vdata})                      
val_out = val_out.assign(intercept = 1.0) # Add intercept


# Calibration slope
cal_slope = smf.GLM(val_out.y_val, 
                    val_out[["intercept", "lp_val"]], 
                    family = smf.families.Binomial())
res_cal_slope = cal_slope.fit()
res_cal_slope_summary = res_cal_slope.summary()
# res_cal_slope_summary
```
</details>

```{python, weak_cal, fig.align='center', warning=FALSE, eval=TRUE}
```

```{python, res_weak_cal, fig.align='center',echo=FALSE}
k = 2
res_slope_cal = pd.DataFrame(
  {"Estimate": round(res_cal_slope.params[1], k),
    " 2.5 %" : round(res_cal_slope.conf_int().iloc[1,0], k),
    "97.5 %" : round(res_cal_slope.conf_int().iloc[1,1], k)},
    index = ["Calibration slope"])

```

```{r, res_weak_cal_r, fig.align='center',echo=FALSE}
kable(py$res_slope_cal) |>
  kable_styling("striped", position = "center") 
```

#### 2.2.3 Moderate calibration
Moderate calibration concerns whether among patients with the same predicted risk, the observed event rate equals the predicted risk. A graphical assessment of calibration is possible with predictions on the x-axis, and the outcome on the y-axis. Perfect predictions should be on the 45° line.  For binary outcomes, the plot contains only 0 and 1 values for the y-axis. Smoothing techniques can be used to estimate
the observed probabilities of the outcome (p(y=1)) in relation to the predicted probabilities, e.g. using the lowess algorithm. The observed probabilities can be also estimated using a secondary logistic regression model using the predicted probability as a covariate. We can assume linearity or a more flexible relation between the covariate and the observed probabilties using, for example, splines. We may however expect that the specific type of smoothing may affect the graphical impression, especially in smaller data sets.

<details>
  <summary>Click to expand code</summary>
```{python, moder_cal, fig.align='center',echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
## Fitting the logistic regression model ------------------
y = rdata["tum_res"]
X_rdata = rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X_rdata = X_rdata.assign(intercept = 1.0)

lrm = smf.GLM(y, X_rdata, family = smf.families.Binomial())
result_lrm = lrm.fit()

# Save predictors of the validation model
X_vdata = vdata         
X_vdata = X_vdata.assign(intercept = 1.0)
X_vdata = X_vdata[["ter_pos_Yes", "preafp_Yes","prehcg_Yes", "sqpost", "reduc10", "intercept"]]

# Save coefficients of the developed model
coeff = result_lrm.params

# Calculating the linear predictor (X*beta)
lp_vdata = np.matmul(X_vdata, coeff)

# Estimated predicted probabilities by the model in the validation data
pred_vdata = result_lrm.predict(X_vdata)

# Create the dataframe including all useful info
# Validation data --
# y_val = outcome of the validation data
# lp_val = linear predictor calculated in the validation data
val_out =  pd.DataFrame({'y_val': vdata["tum_res"], 
                         'lp_val' : lp_vdata,
                         'pred_val' : pred_vdata})                      
val_out = val_out.assign(intercept = 1.0) # Add intercept


# Calibration plot 
# Method used: The actual probability is estimated using a 'secondary' logistic regression model
# using the predicted probabilities as a covariate.
# Non-parametric curve (smooth using lowess) is also used as an alternative method.

pred_val_cal = pd.DataFrame({'pred_val' : pred_vdata})
pred_val_cal['intercept'] = 1.0
moderate_cal = smf.GLM(val_out.y_val, 
                       val_out[["intercept", "pred_val"]], 
                       family = smf.families.Binomial())
res_moderate_cal = moderate_cal.fit()

# Estimated the standard error of the predicted probabilities
# to add confidence bands to the calibration plot estimated using
# a 'secondary' logistic regression model.
# We need: 
# a. matrix of variance and covariance of the 'secondary' logistic model
# res_moderate_cal.cov_params()

# b. estimate the linear predictor as x*beta
lp_cal = np.matmul(val_out[["intercept", "pred_val"]],
                  res_moderate_cal.params)

# Estimating the density 
dlogis = sp.stats.logistic.pdf(lp_cal) # logistic density function = exp(-xb) / (1 + exp(-xb))**2)

# Estimating the standard error of predicted probabilities
#   Formula details are in https://blog.methodsconsultants.com/posts/delta-method-standard-errors/

se_fit = [0] * len(vdata)
for j in range(len(vdata)):
  se_fit[j] = np.dot(dlogis[j], val_out[["intercept", "pred_val"]].loc[j])
  se_fit[j] = np.dot(se_fit[j], res_moderate_cal.cov_params())
  se_fit[j] = np.dot(se_fit[j], val_out[["intercept", "pred_val"]].loc[j].T)
  se_fit[j] = np.dot(se_fit[j], dlogis[j])
se_fit = np.sqrt(se_fit)
# NOTE: I would like to improve and use only matrix operators rather than
# generalizing a single individual case using for loop

# Lowess
lowess = smf.nonparametric.lowess
fit_lowess = lowess(val_out.y_val, 
                    val_out.pred_val, 
                    frac = 2/3,
                    it = 0) # same f and iter parameters as R

# Create df for calibration plot based on secondary log reg
alpha = 0.05
df_cal = pd.DataFrame({
    'obs' :  res_moderate_cal.predict(val_out[["intercept", "pred_val"]]),
    'pred' : val_out.pred_val,
    'se_fit' : se_fit,
    'lower_95' : res_moderate_cal.predict(val_out[["intercept", "pred_val"]]) - sp.stats.norm.ppf(1 - alpha / 2) * se_fit,
    'upper_95' : res_moderate_cal.predict(val_out[["intercept", "pred_val"]]) + sp.stats.norm.ppf(1 - alpha / 2) * se_fit
})

# Sorting
df_cal = df_cal.sort_values(by = ['pred'])

# Calibration plots
p1 = plt.plot(df_cal.pred, df_cal.obs, "--", 
         label = "Logistic", color = "black")
p2 = plt.plot(fit_lowess[:, 0], fit_lowess[:, 1], "-",
         color = "blue", label = "Non parametric")  
plt.legend(loc = "upper left")
p3 = plt.plot(df_cal.pred, df_cal.lower_95, "--", 
         label = "Logistic", color = "black")
p4 = plt.plot(df_cal.pred, df_cal.upper_95, "--", 
         label = "Logistic", color = "black")

plt.xlabel("Predicted probability")
plt.ylabel("Actual probability")
plt.title("Calibration plot")
plt.show()
plt.clf()
plt.cla()
plt.close('all')

# Calibration metrics based on a secondary logistic regression model
k = 3
cal_metrics = pd.DataFrame(
  {'ICI' : np.round(np.mean(abs(df_cal.obs - df_cal.pred)), k),
   'E50' : np.round(np.median(abs(df_cal.obs - df_cal.pred)), k),
   'E90' : np.round(np.quantile(abs(df_cal.obs - df_cal.pred), 
                       0.9, interpolation = 'midpoint'), k)}, 
  index = ["Calibration measures"]
)
```
</details>
```{python moder_cal, fig.align='center', warning=FALSE, eval=TRUE}
```

```{r, res_numcal_r, message=FALSE,warning=FALSE, fig.align='center',include=TRUE, echo=FALSE}
kable(py$cal_metrics) |>
  kable_styling("striped", position = "center") 
```
Calibration measures (i.e., ICI, E50, E90) using a 'secondary' logistic regression to estimate the observed probability of the event indicate good calibration.  
Calibration measures might also be calculated using the non-parametric method (e.g., lowess) to estimate the observed probability of the event.

### 2.3 Overall performance measures
The overall performance measures generally estimate the distance between the predicted outcome and actual outcome.  
We calculate the Brier Score, and the scaled Brier scale (also known as index of prediction accuracy) and the corresponding confidence intervals.

Some confidence intervals are calculated using the bootstrap percentile method.

<details>
  <summary>Click to expand code</summary>
```{python, overall, warning=FALSE, message=FALSE, eval=FALSE, echo=TRUE}
## Fitting the logistic regression model ------------------
# Logistic regression using statsmodels library
y = rdata["tum_res"]
X_rdata = rdata[["ter_pos_Yes", "preafp_Yes", "prehcg_Yes", "sqpost", "reduc10"]]
X_rdata = X_rdata.assign(intercept = 1.0)

lrm = smf.GLM(y, X_rdata, family = smf.families.Binomial())
result_lrm = lrm.fit()

# Create dataframe dev_out and val_out containing all info useful
# to assess prediction performance in the development and in the validation data

# Save estimated predicted probabilities in the development data
pred_rdata = result_lrm.predict(X_rdata)

# Save predictors of the validation model
X_vdata = vdata         
X_vdata = X_vdata.assign(intercept = 1.0)
X_vdata = X_vdata[["ter_pos_Yes", "preafp_Yes","prehcg_Yes", "sqpost", "reduc10", "intercept"]]

# Save estimated predicted probabilities in the validation data
pred_vdata = result_lrm.predict(X_vdata)

# Save coefficients of the developed model
coeff = result_lrm.params

# Calculating the linear predictor (X*beta)
lp_rdata = np.matmul(X_rdata, coeff)
lp_vdata = np.matmul(X_vdata, coeff)

# Create the dataframe including all useful info 
# y_dev = outcome of the development
# lp_dev = linear predictor calculated in the developement data
# pred_dev = estimated predicted probability in the development data
dev_out =  pd.DataFrame({'y_dev': rdata["tum_res"], 
                         'lp_dev' : lp_rdata,
                         'pred_dev' : pred_rdata})                      
dev_out = dev_out.assign(intercept = 1.0) # Add intercept

# Validation data --
# y_val = outcome of the validation data
# lp_val = linear predictor calculated in the validation data
# pred_val = estimated predicted probability in the validation data
val_out =  pd.DataFrame({'y_val': vdata["tum_res"], 
                         'lp_val' : lp_vdata,
                         'pred_val' : pred_vdata})                      
val_out = val_out.assign(intercept = 1.0) # Add intercept


# Brier Score
from sklearn.metrics import brier_score_loss
bs_lrm_dev = brier_score_loss(val_out.y_val, val_out.pred_val)
bs_lrm_val = brier_score_loss(val_out.y_val, val_out.pred_val)


# Scaled brier score
# Develop null model and estimate the Brier Score for the null model - validation data
lrm_null_val = smf.GLM(val_out.y_val, val_out.intercept, 
                   family = smf.families.Binomial())                  
result_lrm_null_val = lrm_null_val.fit()

val_out_null = pd.DataFrame(
  { 
    'y_val' : vdata["tum_res"],
    'lp_null' : [result_lrm_null_val.params[0]] * len(vdata),
    'pred_null' : np.exp([result_lrm_null_val.params[0]] * len(vdata)) / (1 + np.exp([result_lrm_null_val.params[0]] * len(vdata)))
  }
)
bs_lrm_null_val = brier_score_loss(val_out_null.y_val, 
                               val_out_null.pred_null)
 
 
#### Bootstrap percentile confidence intervals
# Bootstrap confidence intervals for development and validation set
# NOTE: I need to understand how to set up a random seed to reproduce
# the same boostrapped data
B = 2000
bdev_out = {}
bval_out = {}
B = 2000
bdev_out_null = {}
bval_out_null = {}
boot_brier_dev = [0] * B
boot_brier_null_dev = [0] * B
boot_brier_val = [0] * B
boot_brier_null_val = [0] * B

for j in range(B): 
  
  bdev_out[j] = sk.utils.resample(dev_out, 
      replace = True, 
      n_samples = len(dev_out)) # bootstrapping development data
      
  bval_out[j] = sk.utils.resample(val_out, 
      replace = True, 
      n_samples = len(val_out)) # bootstrapping validation data                              
# Bootstrap percentile confidence intervals
B = 2000
bval_out_null = {}
boot_brier = [0] * B
boot_brier_null = [0] * B

for j in range(B): 
  bval_out_null[j] = sk.utils.resample(val_out_null, 
      replace = True, 
      n_samples = len(val_out_null))
      
  boot_brier[j] = brier_score_loss(bval_out[j].y_val, 
                                   bval_out[j].pred_val),
  
  boot_brier_null[j] = brier_score_loss(bval_out_null[j].y_val, 
                                        bval_out_null[j].pred_null)
  
scaled_brier_boot = 1 - (np.array(boot_brier) / np.array(boot_brier_null))

# Overall performance results
overall_metrics = np.reshape(
  (bs_lrm,
   np.percentile(boot_brier, q = 2.5),
   np.percentile(boot_brier, q = 97.5), 
   
   1 - bs_lrm / bs_lrm_null,
   np.percentile(scaled_brier_boot, q = 2.5),
   np.percentile(scaled_brier_boot, q = 97.5)),
   
   (2, 3)
)

overall_metrics = pd.DataFrame(overall_metrics, 
                               columns = ["Estimate", "2.5 %", "97.5 %"], 
                               index = ["Brier Score", "Scaled Brier"])
overall_metrics

```
</details>

```{python, overall, fig.align='center', warning=FALSE, eval=TRUE}
```

```{r, res_ov_r, fig.align='center',echo=FALSE}
```


## Goal 3 -  Clinical utility
Discrimination and calibration measures are essential to assess the prediction performance but insufficient to evaluate the potential clinical utility of a risk prediction model for decision making. When new markers are available, clinical utility assessment evaluates whether the extended model helps to improve decision making.  
Clinical utility is measured by the net benefit that includes the number of true positives and the number of false positives.
Generally, in medicine, clinicians accepts to treat a certain number of patients for which interventions are unnecessary to be event free for a given time horizon. So, false negatives (the harm of not being event free for a given time horizon) are more important than false positives (the harm of unnecessary interventions). Thus, net benefit is the number of true positives classifications minus the false positives classifications weighted by a factor related to the harm of not preventing the event versus unnecessary interventions. The weighting is derived from the threshold probability to the event of interest (e.g. residual tumor). For example, a threshold of 20% implies that additional interventions for 4 patients of whom one would have experience the event if untreated is acceptable (thus treating 3 unnecessary patients). This strategy is compared with the strategies of treat all and treat none patients. If overtreatment is harmful, a higher threshold should be used. 

The net benefit is calculated as:  
  
<img src="https://render.githubusercontent.com/render/math?math=%5Chuge%7B%5Cfrac%7BTP%7D%7Bn%7D-%5Cfrac%7BFP%7D%7Bn%7D*%5Cfrac%7Bp_t%7D%7B1-p_t%7D%7D">
  
*TP*=true positive patients   
*FP*=false positive patients  
*n*=number of patients and *p*<sub>t</sub> is the risk threshold.  

  
The decision curve is calculated as follows:
  
1. Choose a time horizon;
2. Specify a risk threshold which reflects the ratio between harms and benefit of an additional intervention;
3. Calculate the number of true positive and false positive given the threshold specified in (2);
4. Calculate the net benefit of the risk prediction model;
5. Plot net benefit on the *y-axis* against the risk threshold on the *x-axis*;
6. Repeat steps 2-4 for each model consideration;
7. Repeat steps 2-4 for the strategy of assuming all patients are treated;
8. Draw a straight line parallel to the *x-axis* at y=0 representing the net benefit associated with the strategy of assuming that all patients are not treated.

Given some thresholds, the model/strategy with higher net benefit represents the one that potentially improves  clinical decision making. However, poor discrimination and calibration lead to lower net benefit.

More details are available in the paper of Vickers et al. [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2577036/).  

<details>
  <summary>Click to expand code</summary>
