---
title: "Validation of logistic regression risk prediction models"
always_allow_html: true
output:
  github_document:
    toc: true
    toc_depth: 5
  keep_text: true
  pandoc_args: --webtex
---
  
## Steps
  
The steps taken in this file are:   
1. To develop a logistic regression risk prediction model.
2. To assess the performance of the model in terms of calibration, discrimination and overall prediction error. We calculate the apparent, internal (optimism-corrected) validation and the external validation.
3. To assess the potential clinical utility the model using decision curve analysis.

### Installing and loading packages and import data

The following libraries are used in this file, the code chunk below will a) check whether you already have them installed, b) install them for you if not already present, and c) load the packages into the session.

```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  fig.retina = 3,
  fig.path = "imgs/",
  echo = FALSE
)
```

```{r, wdlib, message=FALSE, warning=FALSE, echo=TRUE}
# Use pacman to check whether packages are installed, if not load
if (!require("pacman")) install.packages("pacman")
library(pacman)

pacman::p_load(
  rio,
  rms,
  riskRegression,
  plotrix,
  knitr,
  splines,
  kableExtra,
  gtsummary,
  boot,
  tidyverse,
  rsample,
  gridExtra,
  webshot
)

# Import data ------------------
rdata <- readRDS(here::here("Data/rdata.rds"))
vdata <- readRDS(here::here("Data/vdata.rds"))

```

We loaded the development data (rdata) and the validation data (vdata).
More details about development and validation data are provided in the manuscript ["Assessing the performance of prediction models: a framework for some traditional and novel measures"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/) by Steyerberg et al. (2010).

### Descriptive statistics

```{r, import}
rsel <- rdata |> select(-patkey)
vsel <- vdata |> select(-patkey)
rsel$dt <- 1
vsel$dt <- 2
cdata <- rbind.data.frame(rsel, vsel)
cdata$dt <- factor(
  cdata$dt,
  levels = c(1, 2),
  labels = c("Development data", "Validation data")
)
cdata$tum_res <- factor(cdata$tum_res,
                        levels = c(0, 1),
                        labels = c("No", "Yes"))
label(cdata$tum_res) <- "Residual tumor at postchemotherapy resection"
label(cdata$ter_pos) <- "Residual tumor at postchemotherapy resection "
label(cdata$preafp) <- "Prechemotherapy AFP elevated?"
label(cdata$prehcg) <- "Prechemotherapy HCG elevated?"
label(cdata$sqpost) <- "Square root of postchemotherapy mass size"
label(cdata$reduc10) <- "Reduction in mass size per 10%"
label(cdata$lnldhst) <- "Prechemotherapy LDH"
units(cdata$sqpost) <- "mm"
units(cdata$lnldhst) <- "log(LDH/upper limit of local normal value)"
```


```{r tab1_bis, warning=FALSE, message=FALSE}
gtsummary::tbl_summary(
  data = cdata ,
  label = list(
    sqpost ~ "Square root of postchemotherapy mass size (mm)", 
    reduc10 ~ "Reduction in mass size per 10%",
    lnldhst ~ "Prechemotherapy LDH"),
  by = "dt",
  type = all_continuous() ~ "continuous2",
  statistic = all_continuous() ~ c(
     "{mean} ({sd})",
     "{median} ({min}, {max})"
 )
) |>
  gtsummary::as_kable_extra() |>
  kableExtra::kable_styling("striped")

```

## Goal 1 - develop a logistic regression risk prediction model

### 1.1 Check non-linearity of continuous predictors

Here we investigate the potential non-linear relation between continuous predictors (i.e. age and size) and the outcomes. We apply three-knot restricted cubic splines using `rms::rcs()` function (details are given in e.g. Frank Harrell's book 'Regression Model Strategies (second edition)', page 27. We assess the potential non-linearity graphically (plotting the two continuous predictors against the log odds (XB or linear predictor) of both event types. Also, we compare the models with and without splines based on the AIC.

<details>
  <summary>Click to expand code</summary>
```{r,ff, warning=FALSE, fig.align='center', eval=FALSE, echo=TRUE}

# Models without splines
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
options(datadist = NULL)

# Models with splines
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm_rcs <- lrm(tum_res ~ 
                   ter_pos + preafp + prehcg + 
                   rcs(sqpost, 3) + rcs(reduc10, 3) ,
                   data = rdata, x = T, y = T)
options(datadist = NULL)


# print(fit_lrm_rcs)
# print(summary(fit_lrm_rcs))
# print(anova(fit_lrm_rcs))
P_lrm_sqpost_rcs <- Predict(fit_lrm_rcs, "sqpost")
P_lrm_reduc10_rcs <- Predict(fit_lrm_rcs, "reduc10")
options(datadist = NULL)

oldpar <- par(mfrow = c(1, 2), mar = c(5, 5, 1, 1))
par(xaxs = "i", yaxs = "i", las = 1)
# Square root of postchemotherapy mass size
plot(
  P_lrm_sqpost_rcs$sqpost,
  P_lrm_sqpost_rcs$yhat,
  type = "l",
  lwd = 2,
  col = "blue",
  bty = "n",
  xlab = "Square root of postchemotherapy mass size",
  ylab = "log Odds",
  ylim = c(-3, 1.5),
  xlim = c(0, 12)
)
polygon(
  c(P_lrm_sqpost_rcs$sqpost, rev(P_lrm_sqpost_rcs$sqpost)),
  c(P_lrm_sqpost_rcs$upper, rev(P_lrm_sqpost_rcs$lower)),
  col = "grey75",
  border = FALSE
)
par(new = TRUE)
plot(
  P_lrm_sqpost_rcs$sqpost,
  P_lrm_sqpost_rcs$yhat,
  type = "l",
  lwd = 2,
  col = "blue",
  bty = "n",
  xlab = "Square root of postchemotherapy mass size",
  ylab = "log Odds",
  ylim = c(-3, 1.5),
  xlim = c(0, 12)
)

# Reduction in mass size per 10%
plot(
  P_lrm_reduc10_rcs$reduc10,
  P_lrm_reduc10_rcs$yhat,
  type = "l",
  lwd = 2,
  col = "blue",
  bty = "n",
  xlab = "Reduction in mass size per 10%",
  ylab = "log Odds",
  xlim = c(-5, 10),
  ylim = c(-2, 3)
)
polygon(
  c(P_lrm_reduc10_rcs$reduc10, rev(P_lrm_reduc10_rcs$reduc10)),
  c(P_lrm_reduc10_rcs$upper, rev(P_lrm_reduc10_rcs$lower)),
  col = "grey75",
  border = FALSE
)
par(new = TRUE)
plot(
  P_lrm_reduc10_rcs$reduc10,
  P_lrm_reduc10_rcs$yhat,
  type = "l",
  lwd = 2,
  col = "blue",
  bty = "n",
  xlab = "Reduction in mass size per 10%",
  ylab = "log Odds",
  xlim = c(-5, 10),
  ylim = c(-2, 3)
)
options(datadist = NULL)
par(oldpar)
```
</details>

```{r,ff, warning=FALSE, fig.align='center', eval=TRUE}
```

```{r, res_aic, fig.align='center'}
res_AIC <- matrix(c(
  AIC(fit_lrm), AIC(fit_lrm_rcs)
),
byrow = T,
ncol = 2,
nrow = 1,
dimnames =
  list(
    c(
      "Residual tumor at postchemotherapy resection"
    ),
    c(
      "AIC without splines",
      "AIC with splines"
    )
  )
)
kable(res_AIC, row.names = TRUE) |>
  kable_styling("striped", position = "center")
```

Both the graphical comparison and the AIC comparison suggested no relevant departure from linear relations between the continuous predictors (square root of post-chemotherapy mass size and reduction in mass size) and the risk of residual tumor at post-chemotherapy resection.  


### 1.2 Examine the fit of the models

+ Logistic regression risk prediction model without LDH

```{r, summary_lrm1, fig.align='center',warning=FALSE}
dd <- datadist(rdata)
options(datadist = "dd")
options(prType = "html")
fit_lrm1 <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
print(fit_lrm1)
# print(summary(fit_lrm))
options(datadist = NULL)
```

+ Logistic regression risk prediction model with LDH

```{r, summary_lrm2, fig.align='center',warning=FALSE}
dd <- datadist(rdata)
options(datadist = "dd")
options(prType = "html")
fit_lrm2 <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10 + lnldhst,
               data = rdata, x = T, y = T)
print(fit_lrm2)
# print(summary(fit_lrm2))
options(datadist = NULL)
```

The coefficients of the models indicated that positive tumor teratoma, elevated prechemoterapy AFP levels, elevated prechemoterapy HCG levels, postchemotherapy mass size (mm) (expressed in square root) are associated with higher risk to residual tumor after resection. Reduction in mass size is associated with a reduced risk to have residual tumor after resection. 

### 1.5 Plot of predictors vs estimated in the validation data

To get further insight into the effect of the covariates, we plot the covariate values observed in the validation set against the estimated absolute risk of having residual tumor after resection. This gives an idea of the size of the effects.

<details>
  <summary>Click to expand code</summary>
```{r, plot_risk, fig.align='center',warning=FALSE, echo=TRUE, eval=FALSE}
# Models -------------
dd <- datadist(rdata)
options(datadist = "dd")
options(prType = "html")
fit_lrm1 <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
# print(summary(fit_lrm))
options(datadist = NULL)


# Calculate predicted probabilities in the validation data
vdata$pred <- predict(fit_lrm1,
              type = "fitted.ind",
              newdata = vdata)

# Positive teratoma tumor
oldpar <- par(mfrow = c(2, 3))
par(xaxs = "i", yaxs = "i", las = 1)
plot(vdata$ter_pos,
  vdata$pred,
  bty = "n",
  ylim = c(0, 1),
  xlab = "Positive teratoma tumor",
  ylab = "Estimated risk"
)
# Elevated AFP
par(xaxs = "i", yaxs = "i", las = 1)
plot(vdata$preafp,
  vdata$pred,
  bty = "n",
  ylim = c(0, 1),
  xlab = "Elevated AFP levels",
  ylab = "Estimated risk"
)

# Elevated HCG
par(xaxs = "i", yaxs = "i", las = 1)
plot(vdata$prehcg,
  vdata$pred,
  bty = "n",
  ylim = c(0, 1),
  xlab = "Elevated HGC levels",
  ylab = "Estimated risk"
)

# Postchemotherapy mass size
par(xaxs = "i", yaxs = "i", las = 1)
plot(vdata$sqpost,
  vdata$pred,
  bty = "n",
  xlim = c(0, 20),
  ylim = c(0, 1),
  xlab = "Square root of postchemotherapy mass size",
  ylab = "Estimated risk"
)
lines(
  lowess(vdata$sqpost, vdata$pred),
  col = "red",
  lwd = 2
)

# Reduction mass size
par(xaxs = "i", yaxs = "i", las = 1)
plot(vdata$reduc10,
  vdata$pred,
  bty = "n",
  xlim = c(0, 10),
  ylim = c(0, 1),
  xlab = "Reduction mass size",
  ylab = "Estimated risk"
)
lines(
  lowess(vdata$reduc10, vdata$pred),
  col = "red",
  lwd = 2
)

```
</details>

```{r, plot_risk, fig.align='center',warning=FALSE, echo=FALSE, eval=TRUE}
```

## Goal 2 - Assessing performance of a logistic regression risk prediction model

Here we evaluate the performance of the prediction model in terms of discrimination, calibration and overall prediction error. 


### 2.1 Discrimination

We here calculate:  

+ The c-statistic: it is a rank order statistic for predictions against true outcomes. The concordance (c) statistic is the most commonly used performance measure to indicate the discriminative ability of generalized linear regression models. For a binary outcome, c is identical to the area under the Receiver Operating Characteristic (ROC) curve, which plots the sensitivity (true positive rate) against 1 – (false positive rate) for consecutive cutoffs for the probability of an outcome. Accurate predictions discriminate between those with and those without the outcome.

+ Discrimination slope: it can be used as a simple measure for how well subjects with and without the outcome are separated. It is calculated as the absolute difference in average predictions for those with and without the outcome. Visualization is readily possible with a box plot or a histogram, which will show less overlap between those with and those without the outcome for a better discriminating model. 
 

More details are in ["Assessing the performance of prediction models: a framework for some traditional and novel measures"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3575184/) by Steyerberg et al. (2010);

#### 2.2.1 C-statistic 

<details>
  <summary>Click to expand code</summary>
```{r, discrimination,warning=FALSE,message=FALSE,echo=TRUE}

# Models
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
options(datadist = NULL)


# C-index
# Apparent validation
val_rdata <- rcorr.cens(predict(fit_lrm), 
                        S = rdata$tum_res)

# Load function to calculate confidence intervals of c-statistic
source(here::here('Functions/c_stat_ci.R'))
c_dev <- c_stat_ci(val_rdata)


# Internal validation (bootstrapping)
val_opt <- validate(fit = fit_lrm, B = 500)
c_optimism <- round(0.5 * 
                      (1 +
                      val_opt["Dxy","index.corrected"]), 2)

# External validation
val_vdata <- rcorr.cens(
  predict(fit_lrm, newdata = vdata), 
  S = vdata$tum_res)

c_val <- c_stat_ci(val_vdata)

res_c <- rbind(c_dev, c_val)

# Discrimination slope
# Load function to calculate discrimination slope
source(here::here('Functions/discr_slope.R'))
dslope_rdata <- discr_slope(fit_lrm,
                            y = rdata$tum_res,
                            new_data = rdata)
dslope_vdata <- discr_slope(fit_lrm,
                            y = vdata$tum_res,
                            new_data = vdata)

# Bootstrap confidence intervals for discrimination slope
rboot <- bootstraps(rdata, B = 1000)
vboot <- bootstraps(vdata, B = 1000)

discr_slope_boot <- function(split) {
  discr_slope(fit = fit_lrm,
              y = analysis(split)$tum_res,
              new_data = analysis(split))
}

rboot <- rboot |>
  mutate(dslope = map_dbl(splits, discr_slope_boot))

vboot <- vboot |>
  mutate(dslope = map_dbl(splits, discr_slope_boot))

# Optimism-corrected discrimination slope
source(here::here("Functions/internal_cv_lrm.R"))

optim_measures <- bootstrap_cv_lrm(
  db = rdata, 
  outcome = "tum_res", 
  formula = "tum_res ~ 
                  ter_pos + preafp + prehcg + 
                  sqpost + reduc10", 
  formula_score = "tum_res ~ 1")

```
</details>

```{r, res_disc, fig.align='center',echo=FALSE}
alpha <- .05
k <- 2
res_C <- matrix(c(
  c_dev,
  
  c_optimism,
  NA,
  NA,
  
  c_val,
  
  dslope_rdata,
  quantile(rboot$dslope, probs = .025),
  quantile(rboot$dslope, probs = .975),
  
  dslope_vdata,
  quantile(vboot$dslope, probs = .025),
  quantile(vboot$dslope, probs = .975)
  
  int_val["Harrell C corrected"],
  NA,
  NA,
  
  harrell_C_rott5_pgr$concordance,
  harrell_C_rott5_pgr$concordance - 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_rott5_pgr$var) ,
  harrell_C_rott5_pgr$concordance + 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_rott5_pgr$var),
  
  int_val_pgr["Harrell C corrected"],
  NA,
  NA,
  
  harrell_C_gbsg5$concordance,
  harrell_C_gbsg5$concordance - 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_gbsg5$var) ,
  harrell_C_gbsg5$concordance + 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_gbsg5$var),
  
  harrell_C_gbsg5_pgr$concordance,
  harrell_C_gbsg5_pgr$concordance - 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_gbsg5_pgr$var) ,
  harrell_C_gbsg5_pgr$concordance + 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_gbsg5_pgr$var),
  
  Uno_C_rott5$concordance,
  harrell_C_rott5$concordance - 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_rott5$var) ,
  harrell_C_rott5$concordance + 
    qnorm(1 - alpha / 2) * sqrt(harrell_C_rott5$var),
  
  int_val["Harrell C corrected"],
  NA,
  NA,
  
  Uno_C_rott5_pgr$concordance,
  Uno_C_rott5_pgr$concordance - 
    qnorm(1 - alpha / 2) * sqrt(Uno_C_rott5_pgr$var) ,
  Uno_C_rott5_pgr$concordance + 
    qnorm(1 - alpha / 2) * sqrt(Uno_C_rott5_pgr$var),
  
  int_val_pgr["Uno C corrected"],
  NA,
  NA,
  
 
  Uno_C_gbsg5$concordance,
  Uno_C_gbsg5$concordance - 
    qnorm(1 - alpha / 2) * sqrt(Uno_C_gbsg5$var) ,
  Uno_C_gbsg5$concordance + 
    qnorm(1 - alpha / 2) * sqrt(Uno_C_gbsg5$var),
  
  Uno_C_gbsg5_pgr$concordance,
  Uno_C_gbsg5_pgr$concordance - 
    qnorm(1 - alpha / 2) * sqrt(Uno_C_gbsg5_pgr$var) ,
  Uno_C_gbsg5_pgr$concordance + 
    qnorm(1 - alpha / 2) * sqrt(Uno_C_gbsg5_pgr$var)
),
  
  nrow = 2,
  ncol = 18,
  byrow = T,
  
  dimnames = list(
    c("Harrell C", "Uno C"),
    rep(c("Estimate", "Lower .95 ", "Upper .95"), 6)
  )
)
res_C <- round(res_C, 2) # Digits
kable(res_C) |>
  kable_styling("striped", position = "center") |>
  add_header_above(c(" " = 1, 
                     "Apparent" = 3, 
                     "Internal" = 3, 
                     "Apparent + PGR" = 3, 
                     "Internal + PGR" = 3, 
                     "External" = 3, 
                     "External + PGR" = 3))

```

Comments discrimination
### 2.2 Calibration
Describe calibration

##### 2.2.1 Mean calibration

<details>
  <summary>Click to expand code</summary>
```{r, mean_cal, fig.align='center',echo=TRUE, eval=FALSE}
# Models
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
options(datadist = NULL)

# Calibration-in-the-large
lp <- predict(fit_lrm, newdata = vdata)
vdata$y <- as.numeric(vdata$tum_res) - 1 # convert outcome to numeric
cal_intercept <- glm(y  ~ offset(lp), 
                     family = binomial,
                     data = vdata)
intercept_CI <- confint(cal_intercept) # confidence intervals


Obs <- sum(vdata$y)
vdata$pred <- predict(fit_lrm, 
                      newdata = vdata,
                      type = "fitted.ind")
Expct <- sum(vdata$pred)

Obs/Expct
```
</details>

##### 2.2.2 Weak calibration
<details>
  <summary>Click to expand code</summary>
```{r, weak_cal, fig.align='center',echo=TRUE, eval=FALSE}
# Models
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
options(datadist = NULL)

# Calibration slope
cal_slope <- glm(y  ~ lp,
                 family = binomial,
                 data = vdata)
slope_CI <- confint(cal_slope) # Confidence interval
```
</details>


##### 2.2.3 Moderate calibration

<details>
  <summary>Click to expand code</summary>
```{r, moder_cal, fig.align='center',echo=TRUE, eval=FALSE}
# Models
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
options(datadist = NULL)

## Calibration plot
vdata$pred <- predict(fit_lrm,
                      newdata = vdata,
                      type = "fitted.ind")

# Calibration based on a secondary logistic regression
fit_cal <- glm(y ~ pred,
               x = T,
               y = T,
               data = vdata)

cal_obs <- predict(fit_cal,  
                   type = "response",
                   se.fit = TRUE)
alpha <- .05
dt_cal <- cbind.data.frame("obs" = cal_obs$fit,
                           
                           "lower" = 
                             cal_obs$fit - 
                             qnorm(1 - alpha / 2)*cal_obs$se.fit,
                           
                           "upper" = cal_obs$fit + 
                             qnorm(1 - alpha / 2)*cal_obs$se.fit,
                           
                           "pred" = vdata$pred)
dt_cal <- dt_cal[order(dt_cal$pred),]

cal_lowess <- lowess(vdata$pred, vdata$y, iter = 0)
par(xaxs = "i", yaxs = "i", las = 1)
plot(cal_lowess,
     type = "l",
     xlim = c(0, 1),
     ylim = c(0, 1),
     xlab = "Predicted probability",
     ylab = "Actual probability",
     bty = "n",
     lwd = 2,
     main = "Calibration plot")
lines(dt_cal$pred, dt_cal$obs, lwd = 2, lty = 2)
lines(dt_cal$pred, dt_cal$lower, lwd = 2, lty = 3)
lines(dt_cal$pred, dt_cal$upper, lwd = 2, lty = 3)
abline(a = 0, b = 1, col = "gray")
legend("bottomright",
       c("Ideal", "Lowess", "Logistic", "95% confidence interval"),
       lwd = c(1, 2, 2, 2),
       lty = c(1, 1, 2, 3),
       col = c("gray", "black", "black", "black"),
       bty = "n")

# Calibration measures ICI, E50, E90 based on secondary logistic regression
res_calmeas <-
  cbind(
    "ICI" = mean(abs(dt_cal$obs - dt_cal$pred)),
    "E50" = median(abs(dt_cal$obs - dt_cal$pred)),
    "E90" = unname(quantile(abs(dt_cal$obs - dt_cal$pred), probs = .90))
)

res_calmeas


```
</details>

### 2.3 Overall prediction error

We calculate the Brier Score, and the scaled Brier scale (also indicated as index of prediction accuracy) and the corresponding confidence intervals.

Some confidence intervals are calculated using the bootstrap percentile method.
```{r, bootstrap,warning=FALSE}
# Bootstrapping data
set.seed(20201214)
B <- 10 # number of bootstrap samples
rboot <- bootstraps(rdata, times = B)
vboot <- bootstraps(vdata, times = B)
```

```{r, overall, warning=FALSE}

# Models -------------------
# Models
dd <- datadist(rdata, adjto.cat = "first")
options(datadist = "dd")
fit_lrm <- lrm(tum_res ~ 
               ter_pos + preafp + prehcg + 
               sqpost + reduc10,
               data = rdata, x = T, y = T)
options(datadist = NULL)


# Overall performance measures ----------------

# Development data
score_rdata <- Score(
  list("Development set" = fit_lrm),
  formula = tum_res ~ 1,
  data = rdata,
  conf.int = TRUE,
  metrics = c("auc", "brier"),
  summary = c("ipa"),
  plots = "calibration"
)

# Validation data
score_vdata <- Score(
  list("Validation set" = fit_lrm),
  formula = tum_res ~ 1,
  data = vdata,
  conf.int = TRUE,
  metrics = c("auc", "brier"),
  summary = c("ipa"),
  plots = "calibration"
)

# Bootstrap ------
# Functions to expand data and calculate Brier, IPA and AUC in bootstrap
# samples.
# For Brier and AUC, bootstrap should be computationally faster when
# data has more than 2000 rows (see ?riskRegression::Score).
# Our data has 1000 row so we will need only bootstrap to calculate
# confidence intervals of the scaled Brier (IPA) since
# it is not provided by riskRegression::Score() function.


# Score functions in any bootstrap data
score_boot <- function(split) {
  Score(
    list("csh_validation" = fit_csh),
    formula = Hist(time, status_num) ~ 1,
    cens.model = "km",
    data = analysis(split),
    conf.int = TRUE,
    times = horizon,
    metrics = c("auc", "brier"),
    summary = c("ipa"),
    cause = primary_event,
    plots = "calibration"
  )
}

# Development data
rboot <- rboot |> mutate(
  score = map(splits, score_boot),
  scaled_brier = map_dbl(score, function(x) {
    x$Brier$score[model == "csh_validation"]$IPA
  })
)
# Validation data
vboot <- vboot |> mutate(
  score = map(splits, score_boot),
  scaled_brier = map_dbl(score, function(x) {
    x$Brier$score[model == "csh_validation"]$IPA
  })
)


olpar <- par(mfrow = c(2, 2))
aa <- val.prob(vdata$pred,
               as.numeric(vdata$tum_res) - 1)

zz <- lowess(vdata$pred, as.numeric(vdata$tum_res) - 1, iter = 0)
plot(zz$x, zz$y, 
     type = "l",
     xlim = c(0, 1),
     ylim = c(0, 1),
     bty = "n")

plotCalibration(score_vdata)

```

```{r, res_ov, fig.align='center',echo=FALSE}
# Table overall measures

```

Note: unexpectedly, the point estimate for the Brier score is lower (thus better) and for the scaled Brier score is higher (thus better) in the validation data compared to the development data.

## Goal 3 -  Clinical utility

Clinical utility can be measured by the net benefit and plotted in a decision curve. Details about net benefit, decision curve calculation and interpretation are provided in the manuscript (see also the appendix) and its references.

<details>
  <summary>Click to expand code</summary>
```{r, dca, message=FALSE,warning=FALSE, fig.align='center',echo=TRUE,eval=FALSE}
# Run the stdca function to calculate the net benefit and the elements needed to develop decision curve analysis
source(here::here("R/stdca.R"))

# Models ------------------------------
fit_csh <- CSC(Hist(time, status_num) ~
age + size +
  ncat + hr_status,
data = rdata,
fitter = "cph"
)

# useful objects
primary_event <- 1 # Set to 2 if cause 2 was of interest
horizon <- 5 # Set time horizon for prediction (here 5 years)

# Development data
# calculation estimated risk
rdata$pred5 <- predictRisk(fit_csh,
  cause = primary_event,
  newdata = rdata,
  times = horizon
)
rdata <- as.data.frame(rdata)
dca_rdata <- stdca(
  data = rdata,
  outcome = "status_num",
  ttoutcome = "time",
  timepoint = horizon,
  predictors = "pred5",
  xstop = 0.35,
  ymin = -0.01,
  graph = FALSE,
  cmprsk = TRUE
)
# Decision curves plot
oldpar <- par(
  xaxs = "i",
  yaxs = "i",
  las = 1,
  mar = c(6.1, 5.8, 4.1, 2.1),
  mgp = c(4.25, 1, 0)
)
plot(dca_rdata$net.benefit$threshold,
  dca_rdata$net.benefit$pred5,
  type = "l",
  lwd = 2,
  lty = 1,
  xlab = "",
  ylab = "Net Benefit",
  xlim = c(0, 0.5),
  ylim = c(-0.10, 0.10),
  bty = "n",
  xaxt = "n"
)
legend("topright",
  c("Treat all", "Treat none", "Prediction model"),
  lwd = c(2, 2, 2),
  lty = c(1, 2, 1),
  col = c("darkgray", "black", "black"),
  bty = "n"
)
lines(dca_rdata$net.benefit$threshold,
  dca_rdata$net.benefit$none,
  type = "l",
  lwd = 2,
  lty = 4
)
lines(dca_rdata$net.benefit$threshold,
  dca_rdata$net.benefit$all,
  type = "l",
  lwd = 2,
  col = "darkgray"
)
axis(1,
  at = c(0, 0.1, 0.2, 0.3, 0.4, 0.5)
)
axis(1,
  pos = -0.145,
  at = c(0.1, 0.2, 0.3, 0.4, 0.5),
  labels = c("1:9", "1:4", "3:7", "2:3", "1:1")
)
mtext("Threshold probability", 1, line = 2)
mtext("Harm to benefit ratio", 1, line = 5)
title("Development data")
par(oldpar)


# Validation data
# Predicted probability calculation
vdata$pred5 <- predictRisk(fit_csh,
  cause = primary_event,
  newdata = vdata,
  times = horizon
)
vdata <- as.data.frame(vdata)
# Run decision curve analysis
# Development data
# Model without PGR
dca_vdata <- stdca(
  data = vdata,
  outcome = "status_num",
  ttoutcome = "time",
  timepoint = 5,
  predictors = "pred5",
  xstop = 0.45,
  ymin = -0.01,
  graph = FALSE,
  cmprsk = TRUE
)
# Decision curves plot
oldpar <- par(
  xaxs = "i",
  yaxs = "i",
  las = 1,
  mar = c(6.1, 5.8, 4.1, 2.1),
  mgp = c(4.25, 1, 0)
)
plot(dca_vdata$net.benefit$threshold,
  dca_vdata$net.benefit$pred5,
  type = "l",
  lwd = 2,
  lty = 1,
  xlab = "",
  ylab = "Net Benefit",
  xlim = c(0, 0.5),
  ylim = c(-0.10, 0.10),
  bty = "n",
  xaxt = "n"
)
lines(dca_vdata$net.benefit$threshold,
  dca_vdata$net.benefit$none,
  type = "l",
  lwd = 2,
  lty = 4
)
lines(dca_vdata$net.benefit$threshold,
  dca_vdata$net.benefit$all,
  type = "l",
  lwd = 2,
  col = "darkgray"
)
legend("topright",
  c("Treat all", "Treat none", "Prediction model"),
  lwd = c(2, 2, 2),
  lty = c(1, 2, 1),
  col = c("darkgray", "black", "black"),
  bty = "n"
)
axis(1,
  at = c(0, 0.1, 0.2, 0.3, 0.4, 0.5)
)
axis(1,
  pos = -0.145,
  at = c(0.1, 0.2, 0.3, 0.4, 0.5),
  labels = c("1:9", "1:4", "3:7", "2:3", "1:1")
)
mtext("Threshold probability", 1, line = 2)
mtext("Harm to benefit ratio", 1, line = 5)
title("Validation data")
par(oldpar)
```
</details>

```{r, dca, message=FALSE,warning=FALSE, fig.align='center',echo=FALSE,eval=TRUE}
```

If we choose a threshold of 20\%, the model had a net benefit of 0.011 in the development data. In the validation data, the model had a net benefit of 0.014 choosing a threshold of 20\%. 

## Reproducibility ticket

```{r repro_ticket, echo=TRUE}
sessionInfo()
```
